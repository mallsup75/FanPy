{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries if not already present\n",
        "!pip install azure-storage-blob pandas networkx matplotlib\n",
        "\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import io\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Connect to Azure Blob Storage\n",
        "connection_string = \"DefaultEndpointsProtocol=https;AccountName=sg092620240215;AccountKey=+PaTF6WCZ0NY63Hni1XIWRJfWsnTI7QJCLVP0f1OXUoVzJyl0AcE4h2Pe1b7ZbgldGkDDFA0j9iK+AStvU4auA==;EndpointSuffix=core.windows.net\"\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "silver_client = blob_service_client.get_container_client(\"silver\")\n",
        "\n",
        "# Find the latest merged_rosters_players_*.csv\n",
        "latest_blob = None\n",
        "latest_time = None\n",
        "print(\"Checking for merged_rosters_players_ files:\")\n",
        "for blob in silver_client.list_blobs(name_starts_with=\"merged_rosters_players_\"):\n",
        "    print(f\"Found: {blob.name}\")\n",
        "    try:\n",
        "        parts = blob.name.split(\"_\")\n",
        "        if len(parts) < 5:\n",
        "            print(f\"Skipping {blob.name}: Not enough parts\")\n",
        "            continue\n",
        "        timestamp_str = f\"{parts[-2]}_{parts[-1].replace('.csv', '')}\"\n",
        "        timestamp = datetime.strptime(timestamp_str, \"%Y%m%d_%H%M%S\")\n",
        "        if latest_time is None or timestamp > latest_time:\n",
        "            latest_time = timestamp\n",
        "            latest_blob = blob.name\n",
        "    except ValueError as e:\n",
        "        print(f\"Skipping {blob.name}: Invalid timestamp ({e})\")\n",
        "        continue\n",
        "\n",
        "if not latest_blob:\n",
        "    raise ValueError(\"No merged_rosters_players_*.csv files found in silver\")\n",
        "\n",
        "# Load the latest CSV\n",
        "print(f\"Loading {latest_blob}\")\n",
        "blob_client = silver_client.get_blob_client(latest_blob)\n",
        "blob_data = blob_client.download_blob().readall().decode(\"utf-8\")\n",
        "df = pd.read_csv(io.StringIO(blob_data))\n",
        "\n",
        "# Optional: Limit rows for clarity (uncomment if needed)\n",
        "# df = df.head(50)\n",
        "\n",
        "# Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes and edges\n",
        "for index, row in df.iterrows():\n",
        "    player_name = row['name']\n",
        "    team_name = row['teamName']\n",
        "    position = row['position']\n",
        "    \n",
        "    G.add_node(player_name, layer='name')\n",
        "    G.add_node(team_name, layer='team')\n",
        "    G.add_node(position, layer='position')\n",
        "    \n",
        "    G.add_edge(player_name, team_name)  # Name -> Team (right)\n",
        "    G.add_edge(position, player_name)   # Position -> Name (left)\n",
        "\n",
        "# Define positions with aligned spacing\n",
        "pos = {}\n",
        "name_nodes = [n for n, d in G.nodes(data=True) if d['layer'] == 'name']\n",
        "team_nodes = set([n for n, d in G.nodes(data=True) if d['layer'] == 'team'])\n",
        "position_nodes = set([n for n, d in G.nodes(data=True) if d['layer'] == 'position'])\n",
        "\n",
        "# Calculate spacing based on number of name nodes\n",
        "y_spacing = 2.0\n",
        "max_height = 50\n",
        "height = min(len(name_nodes) * y_spacing, max_height)\n",
        "\n",
        "# Assign positions for name nodes (center)\n",
        "for i, node in enumerate(name_nodes):\n",
        "    pos[node] = (0, i * y_spacing)\n",
        "\n",
        "# Align team and position nodes to their connected name nodes\n",
        "for player_name in name_nodes:\n",
        "    y_pos = pos[player_name][1]\n",
        "    for neighbor in G.successors(player_name):\n",
        "        if neighbor in team_nodes:\n",
        "            pos[neighbor] = (2, y_pos)\n",
        "    for predecessor in G.predecessors(player_name):\n",
        "        if predecessor in position_nodes:\n",
        "            pos[predecessor] = (-2, y_pos)\n",
        "\n",
        "# Draw the graph\n",
        "plt.figure(figsize=(20, height), dpi=80)\n",
        "nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=1500, font_size=10, \n",
        "        font_weight='bold', arrows=True, arrowstyle='->', arrowsize=20)\n",
        "plt.title(\"Player Mapping: Position -> Name -> Team\", pad=20)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the graph locally first\n",
        "local_filename = \"player_mapping.png\"\n",
        "plt.savefig(local_filename, dpi=80, bbox_inches='tight')\n",
        "print(f\"Graph saved locally as {local_filename}\")\n",
        "\n",
        "# Upload to silver container\n",
        "blob_name = f\"player_mapping_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.png\"\n",
        "blob_client = silver_client.get_blob_client(blob_name)\n",
        "with open(local_filename, \"rb\") as f:\n",
        "    blob_client.upload_blob(f, overwrite=True)\n",
        "print(f\"Graph uploaded to silver as {blob_name}\")\n",
        "\n",
        "# Display the graph\n",
        "plt.show()\n",
        "\n",
        "# Clean up local file (optional)\n",
        "os.remove(local_filename)\n",
        "print(f\"Local file {local_filename} removed\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "b73298e2-f1b8-4875-a47a-e543f5595c3b",
              "statement_id": 10,
              "statement_ids": [
                10
              ],
              "state": "submitted",
              "livy_statement_state": "running",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "4",
              "normalized_state": "running",
              "queued_time": "2025-03-07T23:15:58.34582Z",
              "session_start_time": null,
              "execution_start_time": "2025-03-07T23:15:58.4450034Z",
              "execution_finish_time": null,
              "parent_msg_id": "bc5b04f2-a699-40be-b3d6-094ac696691d"
            },
            "text/plain": "StatementMeta(b73298e2-f1b8-4875-a47a-e543f5595c3b, 4, 10, Submitted, Running, Running)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1741388855800
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
