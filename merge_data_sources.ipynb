{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from azure.storage.blob import BlobServiceClient\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import os\n",
        "import io\n",
        "\n",
        "# Connect to Azure Blob Storage\n",
        "connection_string = \"DefaultEndpointsProtocol=https;AccountName=sg092620240215;AccountKey=+PaTF6WCZ0NY63Hni1XIWRJfWsnTI7QJCLVP0f1OXUoVzJyl0AcE4h2Pe1b7ZbgldGkDDFA0j9iK+AStvU4auA==;EndpointSuffix=core.windows.net\"\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "silver_client = blob_service_client.get_container_client(\"silver\")\n",
        "\n",
        "# Find latest all_players_*.csv\n",
        "all_players_blob = None\n",
        "latest_time = None\n",
        "blob_list = silver_client.list_blobs(name_starts_with=\"all_players_\")\n",
        "print(\"Checking for all_players_ files:\")\n",
        "for blob in blob_list:\n",
        "    print(f\"Found: {blob.name}\")\n",
        "    try:\n",
        "        parts = blob.name.split(\"_\")\n",
        "        if len(parts) < 3:\n",
        "            print(f\"Skipping {blob.name}: Not enough parts in filename\")\n",
        "            continue\n",
        "        timestamp_str = f\"{parts[-2]}_{parts[-1].replace('.csv', '')}\"\n",
        "        timestamp = datetime.strptime(timestamp_str, \"%Y%m%d_%H%M%S\")\n",
        "        if latest_time is None or timestamp > latest_time:\n",
        "            latest_time = timestamp\n",
        "            all_players_blob = blob.name\n",
        "    except ValueError as e:\n",
        "        print(f\"Skipping {blob.name}: Invalid timestamp format ({str(e)})\")\n",
        "        continue\n",
        "\n",
        "if not all_players_blob:\n",
        "    raise ValueError(\"No valid all_players_*.csv files found in silver\")\n",
        "\n",
        "# Hardcode flattened_rosters.csv\n",
        "rosters_blob = \"flattened_rosters.csv\"\n",
        "print(f\"Checking for {rosters_blob}:\")\n",
        "if not silver_client.get_blob_client(rosters_blob).exists():\n",
        "    raise ValueError(f\"{rosters_blob} not found in silver\")\n",
        "\n",
        "# Load all_players_*.csv\n",
        "print(f\"Loading {all_players_blob}\")\n",
        "all_players_client = silver_client.get_blob_client(all_players_blob)\n",
        "all_players_data = all_players_client.download_blob().readall().decode(\"utf-8\")\n",
        "df_players = pd.read_csv(io.StringIO(all_players_data))\n",
        "\n",
        "# Load flattened_rosters.csv\n",
        "print(f\"Loading {rosters_blob}\")\n",
        "rosters_client = silver_client.get_blob_client(rosters_blob)\n",
        "rosters_data = rosters_client.download_blob().readall().decode(\"utf-8\")\n",
        "df_rosters = pd.read_csv(io.StringIO(rosters_data))\n",
        "\n",
        "# Merge on 'id'\n",
        "merged_df = pd.merge(df_players, df_rosters, on=\"id\", how=\"inner\")\n",
        "if merged_df.empty:\n",
        "    raise ValueError(\"No matching IDs found between all_players and flattened_rosters\")\n",
        "\n",
        "# Transformations\n",
        "# 1. Adjust salary: if < 1000, multiply by 1,000,000\n",
        "merged_df.loc[merged_df['salary'] < 1000, 'salary'] *= 1000000\n",
        "\n",
        "# 2. Add salary_display column\n",
        "merged_df['salary_display'] = merged_df['salary'].apply(lambda x: f\"${int(x / 1000000)}M\")\n",
        "\n",
        "# 3. Drop the 'status' column\n",
        "merged_df = merged_df.drop(columns=['status'])\n",
        "\n",
        "# Save merged CSV to silver\n",
        "output_blob_name = f\"merged_rosters_players_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "output_blob_client = silver_client.get_blob_client(output_blob_name)\n",
        "merged_csv = merged_df.to_csv(index=False)\n",
        "output_blob_client.upload_blob(merged_csv, overwrite=True)\n",
        "print(f\"Uploaded {output_blob_name} to silver container\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "statement_id": -1,
              "statement_ids": [],
              "state": "waiting",
              "livy_statement_state": null,
              "spark_jobs": null,
              "session_id": null,
              "normalized_state": "waiting",
              "queued_time": "2025-03-06T23:39:16.6439465Z",
              "session_start_time": "2025-03-06T23:39:16.6454023Z",
              "execution_start_time": null,
              "execution_finish_time": null,
              "parent_msg_id": "3bca2f0c-00ae-4401-a2b7-6e96e2af4002"
            },
            "text/plain": "StatementMeta(, , -1, Waiting, , Waiting)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1741193112694
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}